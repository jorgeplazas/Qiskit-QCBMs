{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Radial Basis Function (RBF) Kernel\n",
        "\n",
        "The RBF (Radial Basis Function) kernel is widely used to measure similarity between samples.\n",
        "\n",
        "The kernel is defined as:\n",
        "\n",
        "$$\n",
        "K(x, y) = \\exp\\left(-\\gamma (x - y)^2\\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\gamma$ is the kernel bandwidth parameter\n",
        "- $x, y \\in \\mathbb{R}$\n",
        "\n",
        "This function decays as the squared distance between $x$ and $y$ increases,  \n",
        "meaning points farther apart are considered less similar.\n"
      ],
      "metadata": {
        "id": "oZg7Ro7I3J1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def rbf_kernel(x, y, gamma):\n",
        "    return np.exp(-gamma * (x - y) ** 2)"
      ],
      "metadata": {
        "id": "mMyWjV38xlCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel Matrix\n",
        "\n",
        "Given a space of discrete values (e.g., integers from $0$ to $2^n - 1$),  \n",
        "the **kernel matrix** is a symmetric matrix defined as:\n",
        "\n",
        "$$\n",
        "K_{ij} = \\frac{1}{m} \\sum_{k=1}^{m} \\exp\\left(-\\gamma_k (x_i - x_j)^2\\right)\n",
        "$$\n",
        "\n",
        "This matrix encodes pairwise similarity between elements in the space  \n",
        "and is reused across MMD evaluations for efficiency.\n"
      ],
      "metadata": {
        "id": "1iWtcibj3lC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_kernel_matrix(space: np.ndarray, gammas: np.ndarray) -> np.ndarray:\n",
        "    sq_dists = np.abs(space[:, None] - space[None, :]) ** 2\n",
        "    K = sum(np.exp(-gamma * sq_dists) for gamma in gammas) / len(gammas)\n",
        "    return K"
      ],
      "metadata": {
        "id": "TuU_LglS3iwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel Expectation Value\n",
        "\n",
        "Given two distributions $p(x)$ and $q(x)$ represented as vectors, and a kernel matrix $K$:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[K] = p^T K q\n",
        "$$\n",
        "\n",
        "This expression measures how similar the two distributions are in the feature space induced by the kernel.\n"
      ],
      "metadata": {
        "id": "9mWN00Mv3wlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kernel_expectation(px: np.ndarray, py: np.ndarray, kernel_matrix: np.ndarray) -> float:\n",
        "    return px.T @ kernel_matrix @ py"
      ],
      "metadata": {
        "id": "QZIKKmfC3six"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Squared Maximum Mean Discrepancy (MMD) Loss\n",
        "\n",
        "MMD measures the distance between two probability distributions:\n",
        "\n",
        "$$\n",
        "\\text{MMD}^2(p, q) = \\mathbb{E}[K(x, x')] + \\mathbb{E}[K(y, y')] - 2\\mathbb{E}[K(x, y)]\n",
        "$$\n",
        "\n",
        "In matrix form, using kernel expectations:\n",
        "\n",
        "$$\n",
        "\\text{MMD}^2 = (p - q)^T K (p - q)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "aqd73Zeb34RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mmd_loss(px: np.ndarray, py: np.ndarray, kernel_matrix: np.ndarray) -> float:\n",
        "    diff = px - py\n",
        "    return kernel_expectation(diff, diff, kernel_matrix)"
      ],
      "metadata": {
        "id": "wkuxas4133Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kullback–Leibler (KL) Divergence\n",
        "\n",
        "KL divergence measures how much one probability distribution diverges from another:\n",
        "\n",
        "$$\n",
        "D_{KL}(p \\parallel q) = \\sum_{i} p_i \\log\\left(\\frac{p_i}{q_i}\\right)\n",
        "$$\n",
        "\n",
        "If $q_i = 0$, we skip that term to avoid division by zero.  \n",
        "Often, we use a small $\\epsilon$ to clip values and ensure numerical stability.\n"
      ],
      "metadata": {
        "id": "LB0UdnRs3_tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kl_divergence(p: np.ndarray, q: np.ndarray, eps=1e-12) -> float:\n",
        "    p = np.clip(p, eps, 1.0)\n",
        "    q = np.clip(q, eps, 1.0)\n",
        "    return np.sum(p * np.log(p / q))\n"
      ],
      "metadata": {
        "id": "5LbwReBQ3-o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bitstring ↔ Integer Utilities\n",
        "\n",
        "Used to translate between:\n",
        "\n",
        "- Integer indices $i \\in [0, 2^n)$  \n",
        "- Bitstring representations $b \\in \\{0, 1\\}^n$\n"
      ],
      "metadata": {
        "id": "KF5rrvu34Ia5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def int_to_bitstring(n: int, length: int) -> str:\n",
        "    return format(n, f'0{length}b')\n",
        "\n",
        "def bitstring_to_int(bitstring: str) -> int:\n",
        "    return int(bitstring, 2)"
      ],
      "metadata": {
        "id": "hX7Rb5aF4LQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mapping Probabilities to Bitstrings\n",
        "\n",
        "Quantum circuits produce probability distributions over computational basis states (bitstrings).\n",
        "\n",
        "We often want to extract the bitstring representations of non-zero (or significant) probabilities.\n",
        "\n",
        "Given:\n",
        "- A probability vector `p` of length $2^n$\n",
        "- Each index $i$ corresponds to a bitstring of length $n$\n",
        "\n",
        "We use:\n",
        "\n",
        "$$\n",
        "\\texttt{bitstring}_i = \\text{format}(i, '0nb')\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "n = \\log_2(\\text{len}(p))\n",
        "$$\n",
        "\n",
        "This helps in:\n",
        "- Visualizing which basis states the circuit is predicting\n",
        "- Comparing generated vs valid configurations (e.g., for Bars & Stripes)\n"
      ],
      "metadata": {
        "id": "sSkt6N1x4Uui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def probs_to_bitstrings(prob_vector: np.ndarray, threshold: float = 1e-6) -> list:\n",
        "    n = int(np.log2(len(prob_vector)))\n",
        "    return [int_to_bitstring(i, n) for i, p in enumerate(prob_vector) if p > threshold]"
      ],
      "metadata": {
        "id": "Yu0Dx9SO4OJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chi Validity Score\n",
        "\n",
        "If $S$ is a set of generated samples and $V$ is a set of valid bitstrings:\n",
        "\n",
        "$$\n",
        "\\chi = \\frac{\\text{# of valid samples in } S}{\\text{total samples}}\n",
        "$$\n",
        "\n",
        "This evaluates how well a generative model captures a constrained distribution (e.g., Bars & Stripes).\n",
        "\n",
        "This metric tells us how well a model **respects dataset constraints**, especially when only a subset of $2^n$ basis states are valid.\n"
      ],
      "metadata": {
        "id": "tShA9i874Xre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_chi(samples: list, valid_bitstrings: list) -> float:\n",
        "    return np.mean([s in valid_bitstrings for s in samples])"
      ],
      "metadata": {
        "id": "D4uicWGg4ULj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability Normalization & Stable Logarithm\n",
        "\n",
        "### 1. Probability Normalization\n",
        "\n",
        "Due to floating-point precision issues, a probability vector may **not sum to 1 exactly**.\n",
        "\n",
        "To fix this, we normalize each probability value:\n",
        "\n",
        "$$\n",
        "p_i \\leftarrow \\frac{p_i}{\\sum_j p_j}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Safe Logarithm (Avoiding `log(0)`)\n",
        "\n",
        "When computing metrics like **KL Divergence** or **Entropy**, we often need $\\log(p_i)$.  \n",
        "But:\n",
        "\n",
        "- $\\log(0) = -\\infty$ → undefined  \n",
        "- Causes `NaN` or `inf` in calculations\n",
        "\n",
        "So we apply a small epsilon ($\\epsilon$) to avoid zero:\n",
        "\n",
        "$$\n",
        "\\log(p_i) \\rightarrow \\log(\\max(p_i, \\epsilon))\n",
        "$$\n",
        "\n",
        "A typical choice is:\n",
        "\n",
        "$$\n",
        "\\epsilon = 10^{-12}\n",
        "$$\n",
        "\n",
        "This ensures **numerical stability** and avoids invalid values during computation.\n"
      ],
      "metadata": {
        "id": "3eK2T2re48r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_probs(probs: np.ndarray) -> np.ndarray:\n",
        "    total = np.sum(probs)\n",
        "    return probs / total if total > 0 else probs\n",
        "\n",
        "def safe_log(x: np.ndarray, eps=1e-12) -> np.ndarray:\n",
        "    return np.log(np.clip(x, eps, None))"
      ],
      "metadata": {
        "id": "mcCwxAj84fPs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
